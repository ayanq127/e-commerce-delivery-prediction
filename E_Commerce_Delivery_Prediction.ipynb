{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574fa178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88974cd-fe82-452e-a1a9-6c83bbb840b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\ADMIN\\OneDrive\\Desktop\\E-commerce Product Delivery Prediction\\E_Commerce.csv\"\n",
    ")\n",
    "df = df.sample(frac=0.5, random_state=42)\n",
    "\n",
    "print(df.shape)\n",
    "df.info()\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec376304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(y=df[\"Discount_offered\"])\n",
    "plt.title(\"Box Plot - Discount Offered\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"discount_ratio\"] = df[\"Discount_offered\"] / (df[\"Cost_of_the_Product\"] + 1)\n",
    "\n",
    "\n",
    "df[\"weight_category\"] = pd.cut(\n",
    "    df[\"Weight_in_gms\"],\n",
    "    bins=[0, 1000, 3000, 10000],\n",
    "    labels=[0, 1, 2]\n",
    ")\n",
    "\n",
    "\n",
    "df[\"weight_category\"] = df[\"weight_category\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfbfea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We engineered new features to improve model performance. The discount ratio captures the relative impact of discounts,\n",
    " while weight category groups products into logistical weight ranges.\n",
    "The categorical weight feature was converted into numeric format to ensure model compatibility.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c926c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Warehouse_block\",\n",
    "    \"Mode_of_Shipment\",\n",
    "    \"Product_importance\",\n",
    "    \"Gender\"\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df[col] = le.fit_transform(df[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2329831",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "sns.countplot(\n",
    "    x=\"weight_category\",\n",
    "    hue=\"Reached.on.Time_Y.N\",\n",
    "    data=df\n",
    ")\n",
    "plt.title(\"Weight Category vs Delivery Status\")\n",
    "plt.xlabel(\"Weight Category (0=Light, 1=Medium, 2=Heavy)\")\n",
    "plt.ylabel(\"Order Count\")\n",
    "plt.legend(title=\"Delivery Status\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"Reached.on.Time_Y.N\", \"ID\"], axis=1)\n",
    "y = df[\"Reached.on.Time_Y.N\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b38c038",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m scaler = \u001b[43mStandardScaler\u001b[49m()\n\u001b[32m      2\u001b[39m X_train_scaled = scaler.fit_transform(X_train)\n\u001b[32m      3\u001b[39m X_test_scaled = scaler.transform(X_test)\n",
      "\u001b[31mNameError\u001b[39m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af6a0135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stratify ensures that both training and testing datasets have the same proportion \\nof classes as the original dataset, preventing bias during model training.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Stratify ensures that both training and testing datasets have the same proportion \n",
    "of classes as the original dataset, preventing bias during model training.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31710836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_tr, X_te, y_tr, y_te, name):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_te)\n",
    "\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(name)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"=\"*40)\n",
    "    print(classification_report(y_te, y_pred))\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef46ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=7,\n",
    "    weights=\"distance\"\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa89cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_acc = evaluate_model(\n",
    "    lr, X_train_scaled, X_test_scaled,\n",
    "    y_train, y_test,\n",
    "    \"Logistic Regression\"\n",
    ")\n",
    "\n",
    "knn_acc = evaluate_model(\n",
    "    knn, X_train_scaled, X_test_scaled,\n",
    "    y_train, y_test,\n",
    "    \"KNN\"\n",
    ")\n",
    "\n",
    "rf_acc = evaluate_model(\n",
    "    rf, X_train, X_test,\n",
    "    y_train, y_test,\n",
    "    \"Random Forest\"\n",
    ")\n",
    "\n",
    "xgb_acc = evaluate_model(\n",
    "    xgb, X_train, X_test,\n",
    "    y_train, y_test,\n",
    "    \"XGBoost\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b53ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n FINAL MODEL ACCURACY COMPARISON\")\n",
    "print(f\"Logistic Regression Accuracy : {lr_acc:.4f}\")\n",
    "print(f\"KNN Accuracy                : {knn_acc:.4f}\")\n",
    "print(f\"Random Forest Accuracy      : {rf_acc:.4f}\")\n",
    "print(f\"XGBoost Accuracy            : {xgb_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"rf\", rf),\n",
    "        (\"xgb\", xgb)\n",
    "    ],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "\n",
    "voting.fit(X_train, y_train)\n",
    "\n",
    "y_pred_vote = voting.predict(X_test)\n",
    "\n",
    "print(\"\\nVoting Classifier (RF + XGB)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_vote))\n",
    "print(classification_report(y_test, y_pred_vote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A Voting Classifier combines predictions from multiple models.\n",
    "In this project, Random Forest and XGBoost were combined using soft voting to improve prediction accuracy\n",
    "and reduce individual model errors.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "965da458",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mjoblib\u001b[49m.dump(voting, \u001b[33m\"\u001b[39m\u001b[33mdelivery_model.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m joblib.dump(scaler, \u001b[33m\"\u001b[39m\u001b[33mscaler.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Model and scaler saved successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "joblib.dump(voting, \"delivery_model.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "print(\" Model and scaler saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_order = X.columns.tolist()\n",
    "\n",
    "joblib.dump(feature_order, \"feature_order.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Despite applying advanced ensemble methods, accuracy stabilized around 67% due to inherent data limitations and overlapping features.\n",
    " This behavior is expected in real-world logistics prediction problems.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ccce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DETAILED MODEL EVALUATION FOR EACH MACHINE LEARNING MODEL\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    confusion_matrix, \n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOGISTIC REGRESSION EVALUATION\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"LOGISTIC REGRESSION - DETAILED EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "y_proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_precision = precision_score(y_test, y_pred_lr)\n",
    "lr_recall = recall_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "lr_auc = roc_auc_score(y_test, y_proba_lr)\n",
    "\n",
    "print(f\"\\nAccuracy:  {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {lr_precision:.4f}\")\n",
    "print(f\"Recall:    {lr_recall:.4f}\")\n",
    "print(f\"F1-Score:  {lr_f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {lr_auc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "print(cm_lr)\n",
    "print(f\"\\nTrue Negatives:  {cm_lr[0,0]}\")\n",
    "print(f\"False Positives: {cm_lr[0,1]}\")\n",
    "print(f\"False Negatives: {cm_lr[1,0]}\")\n",
    "print(f\"True Positives:  {cm_lr[1,1]}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['Delayed', 'On Time']))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Delayed', 'On Time'],\n",
    "            yticklabels=['Delayed', 'On Time'])\n",
    "axes[0].set_title('Logistic Regression\\nConfusion Matrix', fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Metrics Bar Chart\n",
    "metrics_lr = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "values_lr = [lr_accuracy, lr_precision, lr_recall, lr_f1, lr_auc]\n",
    "axes[1].bar(metrics_lr, values_lr, color='steelblue', edgecolor='black', alpha=0.8)\n",
    "axes[1].set_title('Logistic Regression\\nPerformance Metrics', fontweight='bold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_ylim([0, 1])\n",
    "for i, v in enumerate(values_lr):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# ROC Curve\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr)\n",
    "axes[2].plot(fpr_lr, tpr_lr, color='steelblue', lw=2, label=f'AUC = {lr_auc:.3f}')\n",
    "axes[2].plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "axes[2].set_title('Logistic Regression\\nROC Curve', fontweight='bold')\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eval_logistic_regression.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. K-NEAREST NEIGHBORS (KNN) EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"K-NEAREST NEIGHBORS (KNN) - DETAILED EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Predictions\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "y_proba_knn = knn.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "knn_precision = precision_score(y_test, y_pred_knn)\n",
    "knn_recall = recall_score(y_test, y_pred_knn)\n",
    "knn_f1 = f1_score(y_test, y_pred_knn)\n",
    "knn_auc = roc_auc_score(y_test, y_proba_knn)\n",
    "\n",
    "print(f\"\\nAccuracy:  {knn_accuracy:.4f} ({knn_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {knn_precision:.4f}\")\n",
    "print(f\"Recall:    {knn_recall:.4f}\")\n",
    "print(f\"F1-Score:  {knn_f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {knn_auc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "print(cm_knn)\n",
    "print(f\"\\nTrue Negatives:  {cm_knn[0,0]}\")\n",
    "print(f\"False Positives: {cm_knn[0,1]}\")\n",
    "print(f\"False Negatives: {cm_knn[1,0]}\")\n",
    "print(f\"True Positives:  {cm_knn[1,1]}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn, target_names=['Delayed', 'On Time']))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Greens', ax=axes[0],\n",
    "            xticklabels=['Delayed', 'On Time'],\n",
    "            yticklabels=['Delayed', 'On Time'])\n",
    "axes[0].set_title('KNN\\nConfusion Matrix', fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Metrics Bar Chart\n",
    "metrics_knn = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "values_knn = [knn_accuracy, knn_precision, knn_recall, knn_f1, knn_auc]\n",
    "axes[1].bar(metrics_knn, values_knn, color='mediumseagreen', edgecolor='black', alpha=0.8)\n",
    "axes[1].set_title('KNN\\nPerformance Metrics', fontweight='bold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_ylim([0, 1])\n",
    "for i, v in enumerate(values_knn):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# ROC Curve\n",
    "fpr_knn, tpr_knn, _ = roc_curve(y_test, y_proba_knn)\n",
    "axes[2].plot(fpr_knn, tpr_knn, color='mediumseagreen', lw=2, label=f'AUC = {knn_auc:.3f}')\n",
    "axes[2].plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "axes[2].set_title('KNN\\nROC Curve', fontweight='bold')\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eval_knn.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. RANDOM FOREST EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RANDOM FOREST - DETAILED EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_precision = precision_score(y_test, y_pred_rf)\n",
    "rf_recall = recall_score(y_test, y_pred_rf)\n",
    "rf_f1 = f1_score(y_test, y_pred_rf)\n",
    "rf_auc = roc_auc_score(y_test, y_proba_rf)\n",
    "\n",
    "print(f\"\\nAccuracy:  {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {rf_precision:.4f}\")\n",
    "print(f\"Recall:    {rf_recall:.4f}\")\n",
    "print(f\"F1-Score:  {rf_f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {rf_auc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(cm_rf)\n",
    "print(f\"\\nTrue Negatives:  {cm_rf[0,0]}\")\n",
    "print(f\"False Positives: {cm_rf[0,1]}\")\n",
    "print(f\"False Negatives: {cm_rf[1,0]}\")\n",
    "print(f\"True Positives:  {cm_rf[1,1]}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Delayed', 'On Time']))\n",
    "\n",
    "# Feature Importance\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "feature_names = X.columns.tolist()\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Oranges', ax=axes[0],\n",
    "            xticklabels=['Delayed', 'On Time'],\n",
    "            yticklabels=['Delayed', 'On Time'])\n",
    "axes[0].set_title('Random Forest\\nConfusion Matrix', fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Metrics Bar Chart\n",
    "metrics_rf = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "values_rf = [rf_accuracy, rf_precision, rf_recall, rf_f1, rf_auc]\n",
    "axes[1].bar(metrics_rf, values_rf, color='coral', edgecolor='black', alpha=0.8)\n",
    "axes[1].set_title('Random Forest\\nPerformance Metrics', fontweight='bold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_ylim([0, 1])\n",
    "for i, v in enumerate(values_rf):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# ROC Curve\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)\n",
    "axes[2].plot(fpr_rf, tpr_rf, color='coral', lw=2, label=f'AUC = {rf_auc:.3f}')\n",
    "axes[2].plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "axes[2].set_title('Random Forest\\nROC Curve', fontweight='bold')\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eval_random_forest.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. XGBOOST EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"XGBOOST - DETAILED EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "y_proba_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "xgb_precision = precision_score(y_test, y_pred_xgb)\n",
    "xgb_recall = recall_score(y_test, y_pred_xgb)\n",
    "xgb_f1 = f1_score(y_test, y_pred_xgb)\n",
    "xgb_auc = roc_auc_score(y_test, y_proba_xgb)\n",
    "\n",
    "print(f\"\\nAccuracy:  {xgb_accuracy:.4f} ({xgb_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {xgb_precision:.4f}\")\n",
    "print(f\"Recall:    {xgb_recall:.4f}\")\n",
    "print(f\"F1-Score:  {xgb_f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {xgb_auc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "print(cm_xgb)\n",
    "print(f\"\\nTrue Negatives:  {cm_xgb[0,0]}\")\n",
    "print(f\"False Positives: {cm_xgb[0,1]}\")\n",
    "print(f\"False Negatives: {cm_xgb[1,0]}\")\n",
    "print(f\"True Positives:  {cm_xgb[1,1]}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['Delayed', 'On Time']))\n",
    "\n",
    "# Feature Importance\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': xgb.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "print(feature_importance_xgb.head(10).to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Purples', ax=axes[0],\n",
    "            xticklabels=['Delayed', 'On Time'],\n",
    "            yticklabels=['Delayed', 'On Time'])\n",
    "axes[0].set_title('XGBoost\\nConfusion Matrix', fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Metrics Bar Chart\n",
    "metrics_xgb = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "values_xgb = [xgb_accuracy, xgb_precision, xgb_recall, xgb_f1, xgb_auc]\n",
    "axes[1].bar(metrics_xgb, values_xgb, color='mediumpurple', edgecolor='black', alpha=0.8)\n",
    "axes[1].set_title('XGBoost\\nPerformance Metrics', fontweight='bold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_ylim([0, 1])\n",
    "for i, v in enumerate(values_xgb):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# ROC Curve\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_proba_xgb)\n",
    "axes[2].plot(fpr_xgb, tpr_xgb, color='mediumpurple', lw=2, label=f'AUC = {xgb_auc:.3f}')\n",
    "axes[2].plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "axes[2].set_title('XGBoost\\nROC Curve', fontweight='bold')\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eval_xgboost.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. VOTING CLASSIFIER EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VOTING CLASSIFIER (RF + XGB) - DETAILED EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Predictions\n",
    "y_pred_vote = voting.predict(X_test)\n",
    "y_proba_vote = voting.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "vote_accuracy = accuracy_score(y_test, y_pred_vote)\n",
    "vote_precision = precision_score(y_test, y_pred_vote)\n",
    "vote_recall = recall_score(y_test, y_pred_vote)\n",
    "vote_f1 = f1_score(y_test, y_pred_vote)\n",
    "vote_auc = roc_auc_score(y_test, y_proba_vote)\n",
    "\n",
    "print(f\"\\nAccuracy:  {vote_accuracy:.4f} ({vote_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {vote_precision:.4f}\")\n",
    "print(f\"Recall:    {vote_recall:.4f}\")\n",
    "print(f\"F1-Score:  {vote_f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {vote_auc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_vote = confusion_matrix(y_test, y_pred_vote)\n",
    "print(cm_vote)\n",
    "print(f\"\\nTrue Negatives:  {cm_vote[0,0]}\")\n",
    "print(f\"False Positives: {cm_vote[0,1]}\")\n",
    "print(f\"False Negatives: {cm_vote[1,0]}\")\n",
    "print(f\"True Positives:  {cm_vote[1,1]}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_vote, target_names=['Delayed', 'On Time']))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm_vote, annot=True, fmt='d', cmap='RdYlGn', ax=axes[0],\n",
    "            xticklabels=['Delayed', 'On Time'],\n",
    "            yticklabels=['Delayed', 'On Time'])\n",
    "axes[0].set_title('Voting Classifier\\nConfusion Matrix', fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Metrics Bar Chart\n",
    "metrics_vote = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "values_vote = [vote_accuracy, vote_precision, vote_recall, vote_f1, vote_auc]\n",
    "axes[1].bar(metrics_vote, values_vote, color='darkseagreen', edgecolor='black', alpha=0.8)\n",
    "axes[1].set_title('Voting Classifier\\nPerformance Metrics', fontweight='bold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_ylim([0, 1])\n",
    "for i, v in enumerate(values_vote):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# ROC Curve\n",
    "fpr_vote, tpr_vote, _ = roc_curve(y_test, y_proba_vote)\n",
    "axes[2].plot(fpr_vote, tpr_vote, color='darkseagreen', lw=2, label=f'AUC = {vote_auc:.3f}')\n",
    "axes[2].plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "axes[2].set_title('Voting Classifier\\nROC Curve', fontweight='bold')\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eval_voting_classifier.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. COMPREHENSIVE COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'KNN', 'Random Forest', 'XGBoost', 'Voting Classifier'],\n",
    "    'Accuracy': [lr_accuracy, knn_accuracy, rf_accuracy, xgb_accuracy, vote_accuracy],\n",
    "    'Precision': [lr_precision, knn_precision, rf_precision, xgb_precision, vote_precision],\n",
    "    'Recall': [lr_recall, knn_recall, rf_recall, xgb_recall, vote_recall],\n",
    "    'F1-Score': [lr_f1, knn_f1, rf_f1, xgb_f1, vote_f1],\n",
    "    'ROC-AUC': [lr_auc, knn_auc, rf_auc, xgb_auc, vote_auc]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "comparison_df.to_csv('model_comparison.csv', index=False)\n",
    "print(\"\\n✓ Comparison table saved as 'model_comparison.csv'\")\n",
    "\n",
    "# Visualization - All Metrics Comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "colors_list = ['steelblue', 'mediumseagreen', 'coral', 'mediumpurple', 'darkseagreen']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    axes[idx].bar(comparison_df['Model'], comparison_df[metric], \n",
    "                  color=colors_list, edgecolor='black', alpha=0.8)\n",
    "    axes[idx].set_title(f'{metric} Comparison', fontweight='bold', fontsize=12)\n",
    "    axes[idx].set_ylabel(metric)\n",
    "    axes[idx].set_ylim([0, 1])\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(comparison_df[metric]):\n",
    "        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Overall comparison\n",
    "axes[5].axis('off')\n",
    "best_model_idx = comparison_df['Accuracy'].idxmax()\n",
    "best_model = comparison_df.loc[best_model_idx, 'Model']\n",
    "best_accuracy = comparison_df.loc[best_model_idx, 'Accuracy']\n",
    "\n",
    "axes[5].text(0.5, 0.7, 'BEST MODEL', ha='center', fontsize=20, fontweight='bold')\n",
    "axes[5].text(0.5, 0.5, best_model, ha='center', fontsize=16, color='darkgreen')\n",
    "axes[5].text(0.5, 0.3, f'Accuracy: {best_accuracy*100:.2f}%', ha='center', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eval_comprehensive_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ ALL MODEL EVALUATIONS COMPLETED!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nSaved Files:\")\n",
    "print(\"  • eval_logistic_regression.png\")\n",
    "print(\"  • eval_knn.png\")\n",
    "print(\"  • eval_random_forest.png\")\n",
    "print(\"  • eval_xgboost.png\")\n",
    "print(\"  • eval_voting_classifier.png\")\n",
    "print(\"  • eval_comprehensive_comparison.png\")\n",
    "print(\"  • model_comparison.csv\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3245eedb-f5b7-4c26-b334-cca350a36785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc073f2-0b37-4281-9b99-137f7b97a7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
